{"cells":[{"source":"Commercial banks receive _a lot_ of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this notebook, we will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.\n\nYou have been provided with a small subset of the credit card applications a bank receives. The dataset has been loaded as a Pandas DataFrame for you. You will start from there. \n\nSummarize the data with describe() on cc_apps and then split the data into cc_apps_train and cc_apps_test. \\\nImpute the missing values contained in the dataset. \\\nPreprocess the data by encoding the categorical features. \\\nSegregate features and labels for training and testing: X_train, y_train, X_test, and y_test. Then rescale the training and testing features contained in X_train and X_test: rescaledX_train and rescaledX_test. \\\nTrain a logistic regression classifier logreg on (rescaledX_train, y_train) and evaluate on (rescaledX_test, y_test). \\\nPerform hyperparameter tuning with a GridSearchCV object: grid_model. Once the grid-search process in completed, extract the best model and the best performance score yielded from grid_model.","metadata":{},"id":"35aebf2e-0635-4fef-bc9a-877b6a20fb13","cell_type":"markdown"},{"source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()\n\n#cc_apps.info()\ncc_apps.nunique()\ncc_apps.describe()\n\ncc_apps.drop([11, 13], axis=1, inplace=True)\ncc_apps.head()\n\ncc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size=0.33, random_state=42)\ncc_apps_train.head()\n\ncc_apps_train_nans_replaced = cc_apps_train.replace(\"?\", np.NaN)\ncc_apps_test_nans_replaced = cc_apps_test.replace(\"?\", np.NaN)\n\ncc_apps_train_imputed = cc_apps_train_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\ncc_apps_test_imputed = cc_apps_test_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\n\nfor col in cc_apps_train_imputed.columns:\n    if cc_apps_train_imputed[col].dtypes == \"object\":\n        # Impute with the most frequent value\n        cc_apps_train_imputed = cc_apps_train_imputed.fillna(cc_apps_train_imputed[col].value_counts().index[0])\n        cc_apps_test_imputed = cc_apps_test_imputed.fillna(cc_apps_train_imputed[col].value_counts().index[0])\n        \ncc_apps_train_cat_encoding = pd.get_dummies(cc_apps_train_imputed)\ncc_apps_test_cat_encoding = pd.get_dummies(cc_apps_test_imputed)\n\ncc_apps_test_cat_encoding = cc_apps_test_cat_encoding.reindex(columns=cc_apps_train_cat_encoding.columns, fill_value=0)\n\nX_train, y_train = (cc_apps_train_cat_encoding.iloc[:, :-1].values,cc_apps_train_cat_encoding.iloc[:, [-1]].values)\nX_test, y_test = (cc_apps_test_cat_encoding.iloc[:, :-1].values, cc_apps_test_cat_encoding.iloc[:, [-1]].values)\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)\n\nlogreg = LogisticRegression()\nlogreg.fit(rescaledX_train, y_train)\ny_pred = logreg.predict(rescaledX_test)\n\nprint(confusion_matrix(y_test, y_pred))\n\ntol = [0.01, 0.001, 0.0001]\nmax_iter = [100, 150, 200]\nparam_grid = dict(tol=tol, max_iter=max_iter)\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\nbest_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))\n\nbest_model = grid_model_result.best_estimator_\nprint(\"Accuracy of logistic regression classifier: \", best_model.score(rescaledX_test, y_test))","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":97,"type":"stream"},"1":{"height":95,"type":"dataFrame"}}},"id":"6e86b1e8-a3fa-4b09-982f-795f218bd1a6","cell_type":"code","execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":"[[103   0]\n [  0 125]]\nBest: 1.000000 using {'max_iter': 100, 'tol': 0.01}\nAccuracy of logistic regression classifier:  1.0\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}